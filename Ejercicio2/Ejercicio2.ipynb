{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NV8wZ0MTKjv_"
   },
   "source": [
    "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
    "\n",
    "\n",
    "# Procesamiento de lenguaje natural - CEIA\n",
    "## Alumno: Gonzalo Gontad\n",
    "## Cohorte: 8va\n",
    "\n",
    "### En el presente ejercicio se desarrolló un bot de preguntas y respuestas utilizando SPACY para el procesamiento del corpus, DNN y TF-IDF para la búsqueda de respuestas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCVZakCzAjGN"
   },
   "source": [
    "### 1 - Instalar dependencias\n",
    "Para poder utilizar Spacy en castellano es necesario agregar la librería \"spacy-stanza\" para lematizar palabras en español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kzao7XO9NJAq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 19:45:06.177392: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-17 19:45:06.223602: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-17 19:45:06.224455: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-17 19:45:06.886476: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import string\n",
    "import random \n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras import Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si se quiere entrenar el modelo desde cero, se debe cambiar el valor a True\n",
    "# Si se quiere cargar un modelo ya entrenado, se debe cambiar el valor a False\n",
    "train_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Z_ExOb8uvjqK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gonzalo/anaconda3/envs/nlp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.2.2.json: 140kB [00:00, 57.5MB/s]                    \n",
      "2023-05-17 19:45:09 INFO: Downloading default packages for language: es (Spanish)...\n",
      "2023-05-17 19:45:10 INFO: File exists: /home/gonzalo/stanza_resources/es/default.zip.\n",
      "2023-05-17 19:45:13 INFO: Finished downloading models and saved to /home/gonzalo/stanza_resources.\n",
      "2023-05-17 19:45:13 INFO: Loading these models for language: es (Spanish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ancora  |\n",
      "| mwt       | ancora  |\n",
      "| pos       | ancora  |\n",
      "| lemma     | ancora  |\n",
      "| depparse  | ancora  |\n",
      "| ner       | conll02 |\n",
      "=======================\n",
      "\n",
      "2023-05-17 19:45:13 INFO: Use device: cpu\n",
      "2023-05-17 19:45:13 INFO: Loading: tokenize\n",
      "2023-05-17 19:45:13 INFO: Loading: mwt\n",
      "2023-05-17 19:45:13 INFO: Loading: pos\n",
      "2023-05-17 19:45:13 INFO: Loading: lemma\n",
      "2023-05-17 19:45:13 INFO: Loading: depparse\n",
      "2023-05-17 19:45:14 INFO: Loading: ner\n",
      "2023-05-17 19:45:15 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "import spacy_stanza\n",
    "\n",
    "# Vamos a usar SpaCy-Stanza. Stanza es una librería de NLP de Stanford\n",
    "# SpaCy armó un wrapper para los pipelines y modelos de Stanza\n",
    "# https://stanfordnlp.github.io/stanza/\n",
    "\n",
    "# Descargar el diccionario en español y armar el pipeline de NLP con spacy\n",
    "stanza.download(\"es\")\n",
    "nlp = spacy_stanza.load_pipeline(\"es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wF10RjVMBdV"
   },
   "source": [
    "### 2 - Herramientas de preprocesamiento de datos\n",
    "Entre las tareas de procesamiento de texto en español se implementa:\n",
    "- Quitar acentos y caracteres especiales\n",
    "- Quitar números\n",
    "- Quitar símbolos de puntuación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZxoD2hEExmuX"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# El preprocesamento en castellano requiere más trabajo\n",
    "\n",
    "# Referencia de regex:\n",
    "# https://docs.python.org/3/library/re.html\n",
    "\n",
    "def preprocess_clean_text(text):    \n",
    "    # sacar tildes de las palabras:\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    # quitar caracteres especiales\n",
    "    pattern = r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s]' # [^ : ningún caracter de todos estos\n",
    "    # (termina eliminando cualquier caracter distinto de los del regex)\n",
    "    text = re.sub(pattern, '', text)\n",
    "    pattern = r'[^a-zA-z.,!?/:;\\\"\\'\\s]' # igual al anterior pero sin cifras numéricas\n",
    "    # quitar números\n",
    "    text = re.sub(pattern, '', text)\n",
    "    # quitar caracteres de puntuación\n",
    "    text = ''.join([c for c in text if c not in string.punctuation])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "q-MiMZjh5fu2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'personas ideas estas cosas y los peces y los murcielagos'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"personas Ideas! estás cosas y los peces y los murciélagos\"\n",
    "\n",
    "# Antes de preprocesar los datos se pasa a minúsculas todo el texto\n",
    "preprocess_clean_text(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "I9V-S8JbrtNn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: hola personas ideas estas cosas y los peces y los murcielagos\n",
      "Lematización de cada token:\n",
      "[hola, 'holar']\n",
      "[personas, 'persona']\n",
      "[ideas, 'idea']\n",
      "[estas, 'este']\n",
      "[cosas, 'cosa']\n",
      "[y, 'y']\n",
      "[los, 'el']\n",
      "[peces, 'pez']\n",
      "[y, 'y']\n",
      "[los, 'el']\n",
      "[murcielagos, 'murcielago']\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de como fuciona\n",
    "text = \"hola personas Ideas! estás cosas y los peces y los murciélagos\"\n",
    "\n",
    "# Antes de preprocesar los datos se pasa a minúsculas todo el texto\n",
    "tokens = nlp(preprocess_clean_text(text.lower()))\n",
    "print(\"tokens:\", tokens)\n",
    "print(\"Lematización de cada token:\")\n",
    "for token in tokens:\n",
    "    print([token, token.lemma_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilRbn0KfMm2r"
   },
   "source": [
    "### 3 - Diccionario de entrada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente dataset fue generado utilizando ChatGPT unicamente con el fin de tener un dataset de entrada para el bot. Este puede presentar errores por lo que las respuestas no deben ser tomadas como verdaderas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "NgIGpjymNEH7"
   },
   "outputs": [],
   "source": [
    "# Dataset en formato JSON que representa las posibles preguntas (patterns)\n",
    "# y las posibles respuestas por categoría (tag)\n",
    "# Los \"patterns\" van a formar el corpus para entrenar el clasificador que clasifica en tags\n",
    "# \"respones\" son las respuestas predeterminadas posibles para cada tag\n",
    "dataset = {\"intents\": [\n",
    "            {\n",
    "              \"tag\": \"bienvenida\",\n",
    "              \"patterns\": [\"Hola\", \"¿Cómo estás?\", \"¿Qué tal?\"],\n",
    "              \"responses\": [\"Hola!\", \"Hola, ¿Cómo estás?\", \"¡Bienvenido!\", \"¡Saludos!\"]\n",
    "            },\n",
    "            {\"tag\": \"nombre\",\n",
    "            \"patterns\": [\"¿Cúal es tu nombre?\", \"¿Quién sos?\"],\n",
    "            \"responses\": [\"Soy un chatbot que responde preguntas sobre el libro La Guía del Viajero Intergaláctico\", \"Puedes llamarme Marvin\"]\n",
    "            },\n",
    "            {\n",
    "              \"tag\": \"el_libro\",\n",
    "              \"patterns\": [ \"¿De qué trata el libro?\", \"¿Me puedes contar sobre el libro?\",\"¿De qué trata la historia?\", \"Resumen del libro.\"],\n",
    "              \"responses\": [\"La Guía del Viajero Intergaláctico es una novela de ciencia ficción escrita por Douglas Adams. Cuenta la historia de un humano llamado Arthur Dent y su amigo alienígena, Ford Prefect, quienes se embarcan en una aventura intergaláctica después de que la Tierra es destruida para dar paso a una autopista hiperespacial. La Guía del Viajero Intergaláctico es una herramienta útil para los viajeros galácticos, ya que contiene información sobre los planetas, especies y culturas del universo.\"],\n",
    "            },\n",
    "            {\n",
    "              \"tag\": \"la_guia\",\n",
    "              \"patterns\": [\"¿Qué es la Guía del Viajero Intergaláctico?\", \"¿Me puedes contar sobre la Guía del Viajero Intergaláctico?\"],\n",
    "              \"responses\": [\"La Guía del Viajero Intergaláctico es una herramienta útil para los viajeros galácticos, ya que contiene información sobre los planetas, especies y culturas del universo.\", \"Es una enciclopedia de viajes intergallacticos.\"],\n",
    "            },\n",
    "            {\n",
    "              \"tag\": \"personajes\",\n",
    "              \"patterns\": [\"¿Quiénes son los personajes principales de La Guía del Viajero Intergaláctico?\", \"¿Qué personajes aparecen en el libro?\", \"¿Me puedes hablar sobre los personajes de la historia?\", \"Nombres de personajes.\"],\n",
    "              \"responses\": [\"Los personajes principales son Arthur Dent, un humano que es arrastrado a una aventura intergaláctica; Ford Prefect, un extraterrestre que trabaja para la Guía del Viajero Intergaláctico; Zaphod Beeblebrox, el excéntrico presidente de la galaxia; Marvin, un robot paranoide y deprimido; y Trillian, la única otra persona de la Tierra que sobrevive a su destrucción.\",\"Arthur, Ford, Zaphod, Marvin y Trillian.\"],\n",
    "            },\n",
    "            {\n",
    "              \"tag\": \"humor\",\n",
    "              \"patterns\": [\"¿El libro es divertido?\", \"¿Tiene La Guía del Viajero Intergaláctico un toque de humor?\", \"¿Es el libro una comedia?\"],\n",
    "              \"responses\": [\"Sí, La Guía del Viajero Intergaláctico es una novela de ciencia ficción con un toque de humor absurdo. Douglas Adams tenía un gran sentido del humor y eso se refleja en la historia y en los personajes.\"],\n",
    "            },\n",
    "            {\n",
    "              \"tag\": \"adaptaciones\",\n",
    "              \"patterns\": [\"¿Hay alguna película o serie basada en La Guía del Viajero Intergaláctico?\", \"¿Existe alguna adaptación del libro?\", \"¿Me recomiendas alguna adaptación del libro?\"],\n",
    "              \"responses\": [\"Sí, hay una película de 2005 y una serie de televisión de 1981 basadas en La Guía del Viajero Intergaláctico. También hay adaptaciones de radio y teatro. Cada una de ellas tiene su propio estilo y enfoque, pero todas son divertidas y entretenidas.\"],\n",
    "            },\n",
    "            {\n",
    "              \"tag\": \"autor\",\n",
    "              \"patterns\": [\"¿Quién escribió el libro?\", \"Autor del libro.\"],\n",
    "              \"responses\": [\"Douglas Adams.\", \"Adams fue el autor.\"],\n",
    "            },\n",
    "            {\n",
    "              \"tag\": \"agradecer\",\n",
    "              \"patterns\": [\"Gracias\", \"Muchas gracias\", \"Gracias por la ayuda\"],\n",
    "              \"responses\": [\"¡De nada!\", \"¡No hay problema!\", \"¡Un placer ayudar!\"],\n",
    "            },\n",
    "            {\n",
    "              \"tag\": \"frase\",\n",
    "              \"patterns\": [\"¿Cuál es la frase más famosa del libro?\", \"¿Me puedes decir una frase del libro?\", \"¿Cuál es la frase más famosa de La Guía del Viajero Intergaláctico?\"],\n",
    "              \"responses\": [\"No entiendo nada, pero soy capaz de explicártelo.\", \"La respuesta a la vida, el universo y todo lo demás es 42.\"],\n",
    "            },\n",
    "            {    \n",
    "              \"tag\": \"despedida\",    \n",
    "              \"patterns\": [\"Adiós\", \"Hasta luego\", \"Nos vemos\", \"Chau\", \"Hasta pronto\"],\n",
    "              \"responses\": [\"Hasta luego, y gracias por el pescado\"],\n",
    "            }                    \n",
    "\n",
    "]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19PEDmIDfLRu"
   },
   "source": [
    "### 4 - Preprocesamiento y armado del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "b3HP8abHNRk3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9033/2762758537.py:17: UserWarning: Due to multiword token expansion or an alignment issue, the original text has been replaced by space-separated expanded tokens.\n",
      "  tokens = nlp(preprocess_clean_text(pattern.lower()))\n",
      "/tmp/ipykernel_9033/2762758537.py:17: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['resumen', 'de', 'el', 'libro']\n",
      "Entities: []\n",
      "  tokens = nlp(preprocess_clean_text(pattern.lower()))\n",
      "/tmp/ipykernel_9033/2762758537.py:17: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['que', 'es', 'la', 'guia', 'de', 'el', 'viajero', 'intergalactico']\n",
      "Entities: []\n",
      "  tokens = nlp(preprocess_clean_text(pattern.lower()))\n",
      "/tmp/ipykernel_9033/2762758537.py:17: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['me', 'puedes', 'contar', 'sobre', 'la', 'guia', 'de', 'el', 'viajero', 'intergalactico']\n",
      "Entities: []\n",
      "  tokens = nlp(preprocess_clean_text(pattern.lower()))\n",
      "/tmp/ipykernel_9033/2762758537.py:17: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['quienes', 'son', 'los', 'personajes', 'principales', 'de', 'la', 'guia', 'de', 'el', 'viajero', 'intergalactico']\n",
      "Entities: []\n",
      "  tokens = nlp(preprocess_clean_text(pattern.lower()))\n",
      "/tmp/ipykernel_9033/2762758537.py:17: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['tiene', 'la', 'guia', 'de', 'el', 'viajero', 'intergalactico', 'un', 'toque', 'de', 'humor']\n",
      "Entities: []\n",
      "  tokens = nlp(preprocess_clean_text(pattern.lower()))\n",
      "/tmp/ipykernel_9033/2762758537.py:17: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['hay', 'alguna', 'pelicula', 'o', 'serie', 'basada', 'en', 'la', 'guia', 'de', 'el', 'viajero', 'intergalactico']\n",
      "Entities: []\n",
      "  tokens = nlp(preprocess_clean_text(pattern.lower()))\n",
      "/tmp/ipykernel_9033/2762758537.py:17: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['existe', 'alguna', 'adaptacion', 'de', 'el', 'libro']\n",
      "Entities: []\n",
      "  tokens = nlp(preprocess_clean_text(pattern.lower()))\n",
      "/tmp/ipykernel_9033/2762758537.py:17: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['me', 'recomiendas', 'alguna', 'adaptacion', 'de', 'el', 'libro']\n",
      "Entities: []\n",
      "  tokens = nlp(preprocess_clean_text(pattern.lower()))\n",
      "/tmp/ipykernel_9033/2762758537.py:17: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['autor', 'de', 'el', 'libro']\n",
      "Entities: []\n",
      "  tokens = nlp(preprocess_clean_text(pattern.lower()))\n",
      "/tmp/ipykernel_9033/2762758537.py:17: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['cual', 'es', 'la', 'frase', 'mas', 'famosa', 'de', 'el', 'libro']\n",
      "Entities: []\n",
      "  tokens = nlp(preprocess_clean_text(pattern.lower()))\n",
      "/tmp/ipykernel_9033/2762758537.py:17: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['me', 'puedes', 'decir', 'una', 'frase', 'de', 'el', 'libro']\n",
      "Entities: []\n",
      "  tokens = nlp(preprocess_clean_text(pattern.lower()))\n",
      "/tmp/ipykernel_9033/2762758537.py:17: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['cual', 'es', 'la', 'frase', 'mas', 'famosa', 'de', 'la', 'guia', 'de', 'el', 'viajero', 'intergalactico']\n",
      "Entities: []\n",
      "  tokens = nlp(preprocess_clean_text(pattern.lower()))\n"
     ]
    }
   ],
   "source": [
    "# Datos que necesitaremos, las palabras o vocabulario\n",
    "words = []\n",
    "classes = []\n",
    "doc_X = []\n",
    "doc_y = []\n",
    "\n",
    "# Por cada intención (intents) debemos tomar los patrones que la caracterízan\n",
    "# a esa intención y transformarla a tokens para almacenar en doc_X\n",
    "\n",
    "# El tag de cada intención se almacena como doc_Y (la clase a predecir)\n",
    "# En `words` vamos a guardar el vocabulario\n",
    "# En `class` las posibles clases o tags\n",
    "\n",
    "for intent in dataset[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        # trasformar el patron a tokens\n",
    "        tokens = nlp(preprocess_clean_text(pattern.lower()))\n",
    "        # lematizar los tokens\n",
    "        for token in tokens:            \n",
    "            words.append(token.lemma_)\n",
    "        \n",
    "        doc_X.append(pattern)\n",
    "        doc_y.append(intent[\"tag\"])\n",
    "    \n",
    "    # Agregar el tag a las clases\n",
    "    if intent[\"tag\"] not in classes:\n",
    "        classes.append(intent[\"tag\"])\n",
    "\n",
    "# Elminar duplicados con \"set\" y ordenar el vocubulario y las clases por orden alfabético\n",
    "words = sorted(set(words))\n",
    "classes = sorted(set(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Acy-gcugNbMH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: ['adaptacion', 'adios', 'alguno', 'aparecer', 'autor', 'ayuda', 'basado', 'chau', 'comedia', 'como', 'contar', 'cual', 'de', 'decir', 'divertido', 'el', 'en', 'escribir', 'este', 'existir', 'famoso', 'frase', 'gracias', 'guia', 'haber', 'hablar', 'hasta', 'historia', 'holar', 'humor', 'intergalactico', 'libro', 'luego', 'mas', 'mucho', 'nombre', 'o', 'pelicula', 'personaje', 'poder', 'por', 'principal', 'pronto', 'que', 'quien', 'recomendar', 'resumen', 'ser', 'serie', 'sobre', 'tal', 'tener', 'toque', 'tratar', 'tu', 'uno', 'ver', 'viajero', 'yo']\n",
      "classes: ['adaptaciones', 'agradecer', 'autor', 'bienvenida', 'despedida', 'el_libro', 'frase', 'humor', 'la_guia', 'nombre', 'personajes']\n",
      "doc_X: ['Hola', '¿Cómo estás?', '¿Qué tal?', '¿Cúal es tu nombre?', '¿Quién sos?', '¿De qué trata el libro?', '¿Me puedes contar sobre el libro?', '¿De qué trata la historia?', 'Resumen del libro.', '¿Qué es la Guía del Viajero Intergaláctico?', '¿Me puedes contar sobre la Guía del Viajero Intergaláctico?', '¿Quiénes son los personajes principales de La Guía del Viajero Intergaláctico?', '¿Qué personajes aparecen en el libro?', '¿Me puedes hablar sobre los personajes de la historia?', 'Nombres de personajes.', '¿El libro es divertido?', '¿Tiene La Guía del Viajero Intergaláctico un toque de humor?', '¿Es el libro una comedia?', '¿Hay alguna película o serie basada en La Guía del Viajero Intergaláctico?', '¿Existe alguna adaptación del libro?', '¿Me recomiendas alguna adaptación del libro?', '¿Quién escribió el libro?', 'Autor del libro.', 'Gracias', 'Muchas gracias', 'Gracias por la ayuda', '¿Cuál es la frase más famosa del libro?', '¿Me puedes decir una frase del libro?', '¿Cuál es la frase más famosa de La Guía del Viajero Intergaláctico?', 'Adiós', 'Hasta luego', 'Nos vemos', 'Chau', 'Hasta pronto']\n",
      "doc_y: ['bienvenida', 'bienvenida', 'bienvenida', 'nombre', 'nombre', 'el_libro', 'el_libro', 'el_libro', 'el_libro', 'la_guia', 'la_guia', 'personajes', 'personajes', 'personajes', 'personajes', 'humor', 'humor', 'humor', 'adaptaciones', 'adaptaciones', 'adaptaciones', 'autor', 'autor', 'agradecer', 'agradecer', 'agradecer', 'frase', 'frase', 'frase', 'despedida', 'despedida', 'despedida', 'despedida', 'despedida']\n"
     ]
    }
   ],
   "source": [
    "print(\"words:\", words)\n",
    "print(\"classes:\", classes)\n",
    "print(\"doc_X:\", doc_X)\n",
    "print(\"doc_y:\", doc_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "YI0L2U7IQcvy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario: 59\n"
     ]
    }
   ],
   "source": [
    "# Tamaño del vocabulario\n",
    "print(\"Vocabulario:\", len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "hqBeGKRk_q4r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags: 11\n"
     ]
    }
   ],
   "source": [
    "# Cantidad de tags\n",
    "print(\"Tags:\", len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "vpbJ0guPN2Uq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0] y: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] y: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0] y: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1] y: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0] y: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9033/220604768.py:10: UserWarning: Due to multiword token expansion or an alignment issue, the original text has been replaced by space-separated expanded tokens.\n",
      "  tokens = nlp(preprocess_clean_text(doc.lower()))\n",
      "/tmp/ipykernel_9033/220604768.py:10: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['resumen', 'de', 'el', 'libro']\n",
      "Entities: []\n",
      "  tokens = nlp(preprocess_clean_text(doc.lower()))\n",
      "/tmp/ipykernel_9033/220604768.py:10: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['que', 'es', 'la', 'guia', 'de', 'el', 'viajero', 'intergalactico']\n",
      "Entities: []\n",
      "  tokens = nlp(preprocess_clean_text(doc.lower()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0] y: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9033/220604768.py:10: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['me', 'puedes', 'contar', 'sobre', 'la', 'guia', 'de', 'el', 'viajero', 'intergalactico']\n",
      "Entities: []\n",
      "  tokens = nlp(preprocess_clean_text(doc.lower()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1] y: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9033/220604768.py:10: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['quienes', 'son', 'los', 'personajes', 'principales', 'de', 'la', 'guia', 'de', 'el', 'viajero', 'intergalactico']\n",
      "Entities: []\n",
      "  tokens = nlp(preprocess_clean_text(doc.lower()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0] y: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "X: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1] y: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9033/220604768.py:10: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['tiene', 'la', 'guia', 'de', 'el', 'viajero', 'intergalactico', 'un', 'toque', 'de', 'humor']\n",
      "Entities: []\n",
      "  tokens = nlp(preprocess_clean_text(doc.lower()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0] y: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "X: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0] y: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9033/220604768.py:10: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['hay', 'alguna', 'pelicula', 'o', 'serie', 'basada', 'en', 'la', 'guia', 'de', 'el', 'viajero', 'intergalactico']\n",
      "Entities: []\n",
      "  tokens = nlp(preprocess_clean_text(doc.lower()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0] y: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9033/220604768.py:10: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['existe', 'alguna', 'adaptacion', 'de', 'el', 'libro']\n",
      "Entities: []\n",
      "  tokens = nlp(preprocess_clean_text(doc.lower()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9033/220604768.py:10: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['me', 'recomiendas', 'alguna', 'adaptacion', 'de', 'el', 'libro']\n",
      "Entities: []\n",
      "  tokens = nlp(preprocess_clean_text(doc.lower()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] y: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "X: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9033/220604768.py:10: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['autor', 'de', 'el', 'libro']\n",
      "Entities: []\n",
      "  tokens = nlp(preprocess_clean_text(doc.lower()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "X: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9033/220604768.py:10: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['cual', 'es', 'la', 'frase', 'mas', 'famosa', 'de', 'el', 'libro']\n",
      "Entities: []\n",
      "  tokens = nlp(preprocess_clean_text(doc.lower()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9033/220604768.py:10: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['me', 'puedes', 'decir', 'una', 'frase', 'de', 'el', 'libro']\n",
      "Entities: []\n",
      "  tokens = nlp(preprocess_clean_text(doc.lower()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1] y: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9033/220604768.py:10: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['cual', 'es', 'la', 'frase', 'mas', 'famosa', 'de', 'la', 'guia', 'de', 'el', 'viajero', 'intergalactico']\n",
      "Entities: []\n",
      "  tokens = nlp(preprocess_clean_text(doc.lower()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0] y: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "X: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1] y: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "X: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Transformar doc_X en bag of words por oneHotEncoding\n",
    "# Transformar doc_Y en un vector de clases multicategórico con oneHotEncoding\n",
    "\n",
    "training = []\n",
    "out_empty = [0] * len(classes)\n",
    "\n",
    "for idx, doc in enumerate(doc_X):\n",
    "    # Transformar la pregunta (input) en tokens y lematizar\n",
    "    text = []\n",
    "    tokens = nlp(preprocess_clean_text(doc.lower()))\n",
    "    for token in tokens:\n",
    "        text.append(token.lemma_)\n",
    "\n",
    "    # Transformar los tokens en \"Bag of words\" (arrays de 1 y 0)\n",
    "    bow = []\n",
    "    for word in words:\n",
    "        bow.append(1) if word in text else bow.append(0)\n",
    "    \n",
    "    # Crear el array de salida (class output) correspondiente\n",
    "    output_row = list(out_empty)\n",
    "    output_row[classes.index(doc_y[idx])] = 1\n",
    "\n",
    "    print(\"X:\", bow, \"y:\", output_row)\n",
    "    training.append([bow, output_row])\n",
    "\n",
    "# Mezclar los datos\n",
    "random.shuffle(training)\n",
    "training = np.array(training, dtype=object)\n",
    "# Dividir en datos de entrada y salida\n",
    "train_X = np.array(list(training[:, 0]))\n",
    "train_y = np.array(list(training[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_Hr8QaDfRf3"
   },
   "source": [
    "### 5 - Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "fopb3NqcAGTz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: (59,) output: 11\n"
     ]
    }
   ],
   "source": [
    "# Shape de entrada y salida\n",
    "input_shape = (train_X.shape[1],)\n",
    "output_shape = train_y.shape[1]\n",
    "print(\"input:\", input_shape, \"output:\", output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "xy7tzkwdOZx9"
   },
   "outputs": [],
   "source": [
    "# Entrenamiento del modelo DNN\n",
    "# - Modelo secuencial\n",
    "# - Con regularización\n",
    "# - softmax y optimizador Adam\n",
    "if train_model:\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_shape=input_shape, activation=\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_shape, activation = \"softmax\"))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                optimizer=\"Adam\",\n",
    "                metrics=[\"accuracy\"])\n",
    "    print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "r6hi4EcdOghm"
   },
   "outputs": [],
   "source": [
    "if train_model:\n",
    "    hist = model.fit(x=train_X, y=train_y, epochs=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Pb1GZDjGRP6Q"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "if train_model:\n",
    "    # Entrenamiento\n",
    "    epoch_count = range(1, len(hist.history['accuracy']) + 1)\n",
    "    sns.lineplot(x=epoch_count,  y=hist.history['accuracy'], label='train')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "eTVDnrV0mDRf"
   },
   "outputs": [],
   "source": [
    "# Guardar lo necesario para poder re-utilizar este modelo en el futuro\n",
    "# el vocabulario utilizado (words)\n",
    "# las posibles clases\n",
    "# el modelo\n",
    "import pickle\n",
    "\n",
    "if train_model:\n",
    "    pickle.dump(words, open('words.pkl','wb'))\n",
    "    pickle.dump(classes, open('classes.pkl','wb'))\n",
    "    model.save('chatbot_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF + Similaridad coseno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tf_idf_predictor ():\n",
    "    def __init__(self, corpus):\n",
    "        self.corpus = corpus\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.X = self.vectorizer.fit_transform(self.corpus)\n",
    "\n",
    "    def predict(self, text):\n",
    "        query = self.vectorizer.transform([text])\n",
    "        cs = cosine_similarity(query, self.X)\n",
    "        #print(cs)\n",
    "        return self.corpus[np.argmax(cs)], np.argmax(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(intents_list, intents_json):\n",
    "    tag = intents_list[0] # tomar el tag con el mejor valor softmax\n",
    "    list_of_intents = intents_json[\"intents\"] # intents_json es todo el dataset\n",
    "    for i in list_of_intents: \n",
    "        if i[\"tag\"] == tag: # buscar el tag correspoindiente y dar una respuesta predeterminada aleatoria \n",
    "            result = random.choice(i[\"responses\"])\n",
    "            break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tf_idf = []\n",
    "corpus_tags_tf_idf = []\n",
    "for intent in dataset['intents']:\n",
    "    corpus_tf_idf.extend(intent['patterns'])\n",
    "    corpus_tags_tf_idf.extend([intent['tag']] * len(intent['patterns']))\n",
    "tf_idf_pred = tf_idf_predictor(corpus_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Adams fue el autor.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#obtengo una respuesta\n",
    "question = \"Autor\"\n",
    "most_sim, corpus_tag_idx = tf_idf_pred.predict(question)\n",
    "#busco el tag de la respuesta\n",
    "get_response([corpus_tags_tf_idf[corpus_tag_idx]], dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnD1WvhBfVYR"
   },
   "source": [
    "### 6 - Testing y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "kqBdSGt8Orkm"
   },
   "outputs": [],
   "source": [
    "# convertir texto de entrada del usuario a tokens\n",
    "def text_to_tokens(text):\n",
    "    lemma_tokens = []\n",
    "    tokens = nlp(preprocess_clean_text(text.lower()))\n",
    "    for token in tokens:\n",
    "        lemma_tokens.append(token.lemma_)\n",
    "    #print(lemma_tokens)\n",
    "    return lemma_tokens\n",
    "\n",
    "# transformar el texto de entrada tokenizado a una representación OHE\n",
    "def bag_of_words(text, vocab): \n",
    "    tokens = text_to_tokens(text)\n",
    "    bow = [0] * len(vocab)\n",
    "    for w in tokens: \n",
    "        for idx, word in enumerate(vocab):\n",
    "            if word == w: \n",
    "                bow[idx] = 1\n",
    "    #print(bow)\n",
    "    return np.array(bow)\n",
    "\n",
    "# usar modelo con la entrada en OHE y los labels posibles (tags)\n",
    "def pred_class(text, vocab, labels): \n",
    "    bow = bag_of_words(text, vocab)\n",
    "    words_recognized = sum(bow)\n",
    "\n",
    "    return_list = []\n",
    "    if words_recognized > 0: # sólo si reconoció alguna palabra del vocabulario\n",
    "        result = model.predict(np.array([bow]))[0] # es un array de softmax\n",
    "        thresh = 0.2\n",
    "        # filtrar aquellas entradas menores al umbral `thresh`\n",
    "        y_pred = [[idx, res] for idx, res in enumerate(result) if res > thresh]\n",
    "        # ordenar keys de acuerdo al valor softmax\n",
    "        y_pred.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "        # return_list es una lista de los labels de mayor a menor\n",
    "        for r in y_pred:\n",
    "            return_list.append(labels[r[0]])\n",
    "            #print(labels[r[0]], r[1])\n",
    "\n",
    "    # si no reconoció palabras del vocabulario se devuelve una lista vacía\n",
    "    return return_list\n",
    "\n",
    "# obtener una respuesta predeterminada \n",
    "def get_response(intents_list, intents_json):\n",
    "    tag = intents_list[0] # tomar el tag con el mejor valor softmax\n",
    "    list_of_intents = intents_json[\"intents\"] # intents_json es todo el dataset\n",
    "    for i in list_of_intents: \n",
    "        if i[\"tag\"] == tag: # buscar el tag correspoindiente y dar una respuesta predeterminada aleatoria \n",
    "            result = random.choice(i[\"responses\"])\n",
    "            break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model==False:\n",
    "    #loas tensorflow model\n",
    "    model = tf.keras.models.load_model('chatbot_model.h5')\n",
    "    #load classes pickle\n",
    "    classes = pickle.load(open('classes.pkl','rb'))\n",
    "    #load words pickle\n",
    "    words = pickle.load(open('words.pkl','rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebo ambos metodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gonzalo/anaconda3/envs/nlp/lib/python3.9/site-packages/gradio/deprecation.py:40: UserWarning: `layout` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n",
      "Q: Hola\n",
      "A: ¡Bienvenido!\n",
      "B: Hola!\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Q: ¿Cual es tu nombre?\n",
      "A: Soy un chatbot que responde preguntas sobre el libro La Guía del Viajero Intergaláctico\n",
      "B: Puedes llamarme Marvin\n",
      "1/1 [==============================] - 0s 20ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6776/3251553722.py:4: UserWarning: Due to multiword token expansion or an alignment issue, the original text has been replaced by space-separated expanded tokens.\n",
      "  tokens = nlp(preprocess_clean_text(text.lower()))\n",
      "/tmp/ipykernel_6776/3251553722.py:4: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['quien', 'es', 'el', 'autor', 'de', 'el', 'libro']\n",
      "Entities: []\n",
      "  tokens = nlp(preprocess_clean_text(text.lower()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: ¿Quien es el autor del libro?\n",
      "A: Adams fue el autor.\n",
      "B: Douglas Adams.\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Q: ¿Que es la guia para el viajero intergalactico?\n",
      "A: La Guía del Viajero Intergaláctico es una herramienta útil para los viajeros galácticos, ya que contiene información sobre los planetas, especies y culturas del universo.\n",
      "B: La Guía del Viajero Intergaláctico es una herramienta útil para los viajeros galácticos, ya que contiene información sobre los planetas, especies y culturas del universo.\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Q: ¿De que trata el libro?\n",
      "A: La Guía del Viajero Intergaláctico es una novela de ciencia ficción escrita por Douglas Adams. Cuenta la historia de un humano llamado Arthur Dent y su amigo alienígena, Ford Prefect, quienes se embarcan en una aventura intergaláctica después de que la Tierra es destruida para dar paso a una autopista hiperespacial. La Guía del Viajero Intergaláctico es una herramienta útil para los viajeros galácticos, ya que contiene información sobre los planetas, especies y culturas del universo.\n",
      "B: La Guía del Viajero Intergaláctico es una novela de ciencia ficción escrita por Douglas Adams. Cuenta la historia de un humano llamado Arthur Dent y su amigo alienígena, Ford Prefect, quienes se embarcan en una aventura intergaláctica después de que la Tierra es destruida para dar paso a una autopista hiperespacial. La Guía del Viajero Intergaláctico es una herramienta útil para los viajeros galácticos, ya que contiene información sobre los planetas, especies y culturas del universo.\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Q: ¿Cuales son los personajes principales?\n",
      "A: Los personajes principales son Arthur Dent, un humano que es arrastrado a una aventura intergaláctica; Ford Prefect, un extraterrestre que trabaja para la Guía del Viajero Intergaláctico; Zaphod Beeblebrox, el excéntrico presidente de la galaxia; Marvin, un robot paranoide y deprimido; y Trillian, la única otra persona de la Tierra que sobrevive a su destrucción.\n",
      "B: Los personajes principales son Arthur Dent, un humano que es arrastrado a una aventura intergaláctica; Ford Prefect, un extraterrestre que trabaja para la Guía del Viajero Intergaláctico; Zaphod Beeblebrox, el excéntrico presidente de la galaxia; Marvin, un robot paranoide y deprimido; y Trillian, la única otra persona de la Tierra que sobrevive a su destrucción.\n",
      "1/1 [==============================] - 0s 17ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6776/3251553722.py:4: UserWarning: Due to multiword token expansion or an alignment issue, the original text has been replaced by space-separated expanded tokens.\n",
      "  tokens = nlp(preprocess_clean_text(text.lower()))\n",
      "/tmp/ipykernel_6776/3251553722.py:4: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['puedes', 'decir', 'me', 'alguna', 'frase', 'de', 'el', 'libro']\n",
      "Entities: []\n",
      "  tokens = nlp(preprocess_clean_text(text.lower()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: ¿Puedes decirme alguna frase del libro?\n",
      "A: La respuesta a la vida, el universo y todo lo demás es 42.\n",
      "B: No entiendo nada, pero soy capaz de explicártelo.\n",
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6776/3251553722.py:4: UserWarning: Due to multiword token expansion or an alignment issue, the original text has been replaced by space-separated expanded tokens.\n",
      "  tokens = nlp(preprocess_clean_text(text.lower()))\n",
      "/tmp/ipykernel_6776/3251553722.py:4: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['quien', 'es', 'el', 'escritor', 'de', 'el', 'libro']\n",
      "Entities: []\n",
      "  tokens = nlp(preprocess_clean_text(text.lower()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: ¿quien es el escritor del libro?\n",
      "A: Soy un chatbot que responde preguntas sobre el libro La Guía del Viajero Intergaláctico\n",
      "B: Sí, La Guía del Viajero Intergaláctico es una novela de ciencia ficción con un toque de humor absurdo. Douglas Adams tenía un gran sentido del humor y eso se refleja en la historia y en los personajes.\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Q: ¿Como te llamas?\n",
      "A: Hola, ¿Cómo estás?\n",
      "B: ¡Saludos!\n",
      "1/1 [==============================] - 0s 21ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6776/3251553722.py:4: UserWarning: Due to multiword token expansion or an alignment issue, the original text has been replaced by space-separated expanded tokens.\n",
      "  tokens = nlp(preprocess_clean_text(text.lower()))\n",
      "/tmp/ipykernel_6776/3251553722.py:4: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['cual', 'es', 'la', 'tematica', 'de', 'el', 'libro']\n",
      "Entities: []\n",
      "  tokens = nlp(preprocess_clean_text(text.lower()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: ¿Cual es la tematica del libro?\n",
      "A: No entiendo nada, pero soy capaz de explicártelo.\n",
      "B: No entiendo nada, pero soy capaz de explicártelo.\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Q: ¿Para que sirve la guia?\n",
      "A: Es una enciclopedia de viajes intergallacticos.\n",
      "B: No entiendo nada, pero soy capaz de explicártelo.\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Q: Adios\n",
      "A: Hasta luego, y gracias por el pescado\n",
      "B: Hola, ¿Cómo estás?\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Q: Chau\n",
      "A: Hasta luego, y gracias por el pescado\n",
      "B: Hasta luego, y gracias por el pescado\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Q: Gracias\n",
      "A: ¡Un placer ayudar!\n",
      "B: ¡Un placer ayudar!\n",
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "questions_responses = pd.DataFrame(columns=['question', 'DNN', 'TF-IDF'])\n",
    "\n",
    "def bot_response(human_text):\n",
    "    intents = pred_class(human_text, words, classes)\n",
    "    if len(intents) > 0:\n",
    "        resp_a = get_response(intents, dataset)\n",
    "    else: # si no hubo ningún resultado que supere el umbral\n",
    "        resp_a = \"Perdón, no comprendo la pregunta.\"\n",
    "    most_sim, corpus_tag_idx = tf_idf_pred.predict(human_text)\n",
    "    resp_b = get_response([corpus_tags_tf_idf[corpus_tag_idx]], dataset)\n",
    "    print(\"Q:\", human_text)  \n",
    "    print(\"A:\", resp_a)\n",
    "    print(\"B:\", resp_b)\n",
    "    questions_responses.loc[len(questions_responses)] = [human_text, resp_a, resp_b]\n",
    "    return resp_a, resp_b\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=bot_response,\n",
    "    inputs=[\"textbox\"],\n",
    "    outputs=[\"text\",\"text\"],\n",
    "    layout=\"vertical\",\n",
    "    title=\"Chatbot\",\n",
    "    )\n",
    "\n",
    "\n",
    "iface.launch(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resumen de preguntas y respuestas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>DNN</th>\n",
       "      <th>TF-IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hola</td>\n",
       "      <td>¡Bienvenido!</td>\n",
       "      <td>Hola!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>¿Cual es tu nombre?</td>\n",
       "      <td>Soy un chatbot que responde preguntas sobre el...</td>\n",
       "      <td>Puedes llamarme Marvin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>¿Quien es el autor del libro?</td>\n",
       "      <td>Adams fue el autor.</td>\n",
       "      <td>Douglas Adams.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>¿Que es la guia para el viajero intergalactico?</td>\n",
       "      <td>La Guía del Viajero Intergaláctico es una herr...</td>\n",
       "      <td>La Guía del Viajero Intergaláctico es una herr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>¿De que trata el libro?</td>\n",
       "      <td>La Guía del Viajero Intergaláctico es una nove...</td>\n",
       "      <td>La Guía del Viajero Intergaláctico es una nove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>¿Cuales son los personajes principales?</td>\n",
       "      <td>Los personajes principales son Arthur Dent, un...</td>\n",
       "      <td>Los personajes principales son Arthur Dent, un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>¿Puedes decirme alguna frase del libro?</td>\n",
       "      <td>La respuesta a la vida, el universo y todo lo ...</td>\n",
       "      <td>No entiendo nada, pero soy capaz de explicártelo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>¿quien es el escritor del libro?</td>\n",
       "      <td>Soy un chatbot que responde preguntas sobre el...</td>\n",
       "      <td>Sí, La Guía del Viajero Intergaláctico es una ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>¿Como te llamas?</td>\n",
       "      <td>Hola, ¿Cómo estás?</td>\n",
       "      <td>¡Saludos!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>¿Cual es la tematica del libro?</td>\n",
       "      <td>No entiendo nada, pero soy capaz de explicártelo.</td>\n",
       "      <td>No entiendo nada, pero soy capaz de explicártelo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>¿Para que sirve la guia?</td>\n",
       "      <td>Es una enciclopedia de viajes intergallacticos.</td>\n",
       "      <td>No entiendo nada, pero soy capaz de explicártelo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Adios</td>\n",
       "      <td>Hasta luego, y gracias por el pescado</td>\n",
       "      <td>Hola, ¿Cómo estás?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Chau</td>\n",
       "      <td>Hasta luego, y gracias por el pescado</td>\n",
       "      <td>Hasta luego, y gracias por el pescado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Gracias</td>\n",
       "      <td>¡Un placer ayudar!</td>\n",
       "      <td>¡Un placer ayudar!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question   \n",
       "0                                              Hola  \\\n",
       "1                               ¿Cual es tu nombre?   \n",
       "2                     ¿Quien es el autor del libro?   \n",
       "3   ¿Que es la guia para el viajero intergalactico?   \n",
       "4                           ¿De que trata el libro?   \n",
       "5           ¿Cuales son los personajes principales?   \n",
       "6           ¿Puedes decirme alguna frase del libro?   \n",
       "7                  ¿quien es el escritor del libro?   \n",
       "8                                  ¿Como te llamas?   \n",
       "9                   ¿Cual es la tematica del libro?   \n",
       "10                         ¿Para que sirve la guia?   \n",
       "11                                            Adios   \n",
       "12                                             Chau   \n",
       "13                                          Gracias   \n",
       "\n",
       "                                                  DNN   \n",
       "0                                        ¡Bienvenido!  \\\n",
       "1   Soy un chatbot que responde preguntas sobre el...   \n",
       "2                                 Adams fue el autor.   \n",
       "3   La Guía del Viajero Intergaláctico es una herr...   \n",
       "4   La Guía del Viajero Intergaláctico es una nove...   \n",
       "5   Los personajes principales son Arthur Dent, un...   \n",
       "6   La respuesta a la vida, el universo y todo lo ...   \n",
       "7   Soy un chatbot que responde preguntas sobre el...   \n",
       "8                                  Hola, ¿Cómo estás?   \n",
       "9   No entiendo nada, pero soy capaz de explicártelo.   \n",
       "10    Es una enciclopedia de viajes intergallacticos.   \n",
       "11              Hasta luego, y gracias por el pescado   \n",
       "12              Hasta luego, y gracias por el pescado   \n",
       "13                                 ¡Un placer ayudar!   \n",
       "\n",
       "                                               TF-IDF  \n",
       "0                                               Hola!  \n",
       "1                              Puedes llamarme Marvin  \n",
       "2                                      Douglas Adams.  \n",
       "3   La Guía del Viajero Intergaláctico es una herr...  \n",
       "4   La Guía del Viajero Intergaláctico es una nove...  \n",
       "5   Los personajes principales son Arthur Dent, un...  \n",
       "6   No entiendo nada, pero soy capaz de explicártelo.  \n",
       "7   Sí, La Guía del Viajero Intergaláctico es una ...  \n",
       "8                                           ¡Saludos!  \n",
       "9   No entiendo nada, pero soy capaz de explicártelo.  \n",
       "10  No entiendo nada, pero soy capaz de explicártelo.  \n",
       "11                                 Hola, ¿Cómo estás?  \n",
       "12              Hasta luego, y gracias por el pescado  \n",
       "13                                 ¡Un placer ayudar!  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayatkwp4fYQx"
   },
   "source": [
    "### 7 - Conclusiones\n",
    "En el resumen se puede ver que tanto el predictor mediante redes neuronales como el predictor mediante TF-IDF + Similaridad coseno tienen un desempeño similar.\n",
    "Los principales errores se deben al uso de sinonimos o palabras similares que no estan en el diccionario de entrada, como en el caso de las preguntas 7, 8, 9 y 10.\n",
    "Tambien se pueden ver casos mas graves como el saludo en el punto 11 que  esta en el diccionario de entrada pero el prdictor TF-IDF no obtuvo la respuesta correcta.\n",
    "\n",
    "### 8 - Mejoras\n",
    "Para mejorar el desempeño se podria utilizar un dataset mas grande y con mas variedad de preguntas para cada respuesta y mayor cantidad de sinonimos.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPZeXqdm2EV/WEA+xr24OAL",
   "collapsed_sections": [],
   "name": "2b - bot_dnn_spacy_esp.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
